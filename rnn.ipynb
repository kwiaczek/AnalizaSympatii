{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/file.csv', index_col=0)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((data.tweets, data.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'ChatGPT: Optimizing Language Models for Dialogue https://t.co/K9rKRygYyn @OpenAI'\n",
      "b'neutral'\n",
      "b'Try talking with ChatGPT, our new AI system which is optimized for dialogue. Your feedback will help us improve it. https://t.co/sHDm57g3Kr'\n",
      "b'good'\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in dataset.batch(2).take(1):\n",
    "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "        print(review)\n",
    "        print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    #ogranicznie znakow do 300\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    #usuniecie linkow\n",
    "    X_batch = tf.strings.regex_replace(X_batch, r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", b\" \") \n",
    "    #usuniecie \\n\n",
    "    X_batch = tf.strings.regex_replace(X_batch, r\"\\\\n\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "\n",
    "    y_batch = tf.where(y_batch==\"good\", 2   , tf.where(y_batch==\"neutral\", 1, 0));\n",
    "\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in dataset.batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 11585),\n",
       " (b'ChatGPT', 382),\n",
       " (b'a', 218),\n",
       " (b'to', 201),\n",
       " (b'is', 172),\n",
       " (b'the', 171),\n",
       " (b'and', 156),\n",
       " (b'it', 146),\n",
       " (b'OpenAI', 141),\n",
       " (b'for', 116),\n",
       " (b'I', 113),\n",
       " (b'of', 94),\n",
       " (b'with', 69),\n",
       " (b'in', 67),\n",
       " (b'GPT', 60),\n",
       " (b'AI', 59),\n",
       " (b'about', 56),\n",
       " (b'new', 55),\n",
       " (b'by', 48),\n",
       " (b'that', 45),\n",
       " (b'from', 44),\n",
       " (b'this', 43),\n",
       " (b'on', 42),\n",
       " (b'you', 41),\n",
       " (b'me', 40)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vocab_size = 10000\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocab_size]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "29\n",
      "16\n",
      "26\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
    "for word in b\"OpenAI my about be can\".split():\n",
    "    print(word_to_id.get(word) or vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = dataset.batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  1  33  53 ...   0   0   0]\n",
      " [117 105  12 ...   0   0   0]\n",
      " [  1  33  53 ...   0   0   0]\n",
      " ...\n",
      " [ 81  13   1 ...   0   0   0]\n",
      " [  1   4  31 ...   0   0   0]\n",
      " [117 105  12 ...   0   0   0]], shape=(32, 45), dtype=int64)\n",
      "tf.Tensor([1 2 1 2 0 2 0 2 2 1 0 1 1 1 1 1 1 0 1 1 1 2 1 1 2 1 2 1 1 1 2 2], shape=(32,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for X_batch, y_batch in train_set.take(1):\n",
    "    print(X_batch)\n",
    "    print(y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13/13 [==============================] - 5s 55ms/step - loss: 1.0991 - accuracy: 0.3150\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 1s 56ms/step - loss: 1.0111 - accuracy: 0.5375\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 1s 54ms/step - loss: 0.5750 - accuracy: 0.7850\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 1s 52ms/step - loss: 0.1670 - accuracy: 0.9650\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 1s 53ms/step - loss: 0.0624 - accuracy: 0.9875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(3, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
